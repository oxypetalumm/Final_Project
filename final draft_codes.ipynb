{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import linear_model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing= pd.read_csv(\"Data/housing.csv\")\n",
    "anscombe_df = pd.read_csv(\"Data/anscombe.csv\")\n",
    "print(housing.head())\n",
    "print(\"------\")\n",
    "print(anscombe_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the boxplot of housing_median_age vs median house value\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(housing_df[\"housing_median_age\"], housing_df[\"median_house_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.distplot(housing_df[\"median_house_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.hist(bins=50, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The attributes have different scales. It is recommended to rescale all the attributes.\n",
    "* <b><u>The medium house value</u></b> has a sudden peak around 500000, which is very different from others.\n",
    "* It is recommended to remove these data in training the model.\n",
    "* <b><u>The medium income</u></b> is centered around 3, where the unit is unknown. Probably, 3 means $300,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"ocean_proximity\"].value_counts().plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for median house value vs ocean proximity\n",
    "sns.boxplot(x=\"ocean_proximity\", y=\"median_house_value\", data=housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(housing_df, x_vars=['housing_median_age', 'population', 'households', 'median_income'],y_vars ='median_house_value',hue = 'ocean_proximity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {'<1H OCEAN':0, 'INLAND':1, 'ISLAND':2, 'NEAR BAY':3, 'NEAR OCEAN':4}\n",
    "def map_fea(x):\n",
    "    return map_dict[x]\n",
    "    \n",
    "housing_df[\"ocean_proximity\"] = housing_df[\"ocean_proximity\"].apply(map_fea)\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new categorical feature in order to split the dataset properly\n",
    "* income_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['income_data'] = np.ceil(housing_df['median_income']/1.5)\n",
    "housing_df['income_data'].where(housing_df['income_data']<5, 5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(housing_df.income_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most people has annual income more than $300,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.income_data.value_counts()/len(housing_df.income_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Machine Learning Algorithm\n",
    "* split the data into train and test sets based on the new feature\n",
    "\n",
    "* Method 1: using the package train_test_split without the new feature\n",
    "* Method 2: using the package StratifiedShuffleSplit with the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4train,data4test = train_test_split(housing_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in split.split(housing_df, housing_df['income_data']):\n",
    "    train_set = housing_df.loc[train_index]\n",
    "    test_set  = housing_df.loc[test_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table4income_cat(dataset,df,label):\n",
    "    df[label]=pd.Series(dataset['income_data'].value_counts()/len(dataset['income_data']))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = table4income_cat(train_set,df,'All_set')\n",
    "df = table4income_cat(train_set,df,'train_set_Shuff')\n",
    "df = table4income_cat(test_set,df,'test_set_Shuff')\n",
    "df = table4income_cat(data4train,df,'train_set_split')\n",
    "df = table4income_cat(data4test,df,'test_set_split')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is shown above that the <u><b>StratifiedShuffleSplit</b></u> method works a little bit better as the income category propotions in train data and test data are closer to that in the all dataset.\n",
    "\n",
    "* After split the data into train and test, we delete the newly added feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the new feature\n",
    "for set_ in (housing_df, data4train, data4test, train_set, test_set):\n",
    "    set_.drop(\"income_data\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing4train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing4train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California maping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing4train.plot(kind='scatter', x='longitude', y='latitude', alpha=0.3,\n",
    "         s=housing4train['population']/100, label='population',   # set symbol size on population\n",
    "         c=housing4train['median_house_value'],                  #  set symbol color on house value    \n",
    "         cmap=plt.get_cmap('jet'),      \n",
    "         colorbar=True,\n",
    "         figsize=(10,7))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Population and location(how close to the ocean) affects the housing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(housing_df.corr(),annot=True,fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing4train.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In dataset form instead of map\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corr_matrix.tail(2).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='median_house_value',inplace=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Able to see that the large correlations within median house value and income, total_rooms and house age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data4train.drop(\"median_house_value\",axis=1)\n",
    "y_train = data4train[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data4test.drop(\"median_house_value\",axis=1)\n",
    "y_test = data4test[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for missing data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "def report_missing_data(dataset):\n",
    "    total = dataset.isnull().sum().sort_values(ascending=False)\n",
    "    percent = dataset.isnull().sum()/total \n",
    "        \n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data.plot(kind='bar',y='Total',figsize=(6,4),fontsize=9)\n",
    "    print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_data(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.drop(\"median_house_value\",axis=1)\n",
    "y_train = train_set[\"median_house_value\"].copy()\n",
    "\n",
    "x_test = test_set.drop(\"median_house_value\",axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_data(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['total_bedrooms'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_feature = pd.DataFrame(x_train.isnull().sum().sort_values(ascending=False)).index[0]\n",
    "missing_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only the total bedrooms contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median=x_train[missing_feature].median()\n",
    "x_train[missing_feature]=x_train[missing_feature].replace(np.nan,median)\n",
    "x_train[missing_feature].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median=x_test[missing_feature].median()\n",
    "x_test[missing_feature]=x_test[missing_feature].replace(np.nan,median)\n",
    "x_test[missing_feature].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = x_train.drop(\"ocean_proximity\",axis=1)\n",
    "num_attribs = list(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "#CategoricalEncoder(encoding='onehot-dense')\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "#select columns and transit to array\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,feature_names):\n",
    "        self.feature_names = feature_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.feature_names].values\n",
    "    \n",
    "\n",
    "# build pipelines\n",
    "cat_attribs = ['ocean_proximity']\n",
    "num_attribs = list(housing_num)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "               ('selector',DataFrameSelector(num_attribs)),      \n",
    "               ('std_scaler',StandardScaler()), \n",
    "                ]) \n",
    "\n",
    "# build categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "                  ('selector',DataFrameSelector(cat_attribs)),\n",
    "                  ('cat_encoder',CategoricalEncoder(encoding='onehot-dense')),\n",
    "              ])\n",
    "\n",
    "\n",
    "# concatenate all the transforms using \"FeatureUnion\"\n",
    "pipelines = FeatureUnion(transformer_list=\n",
    "                             [ \n",
    "                              ('num_pipeline',num_pipeline),\n",
    "                              ('cat_pipeline',cat_pipeline),\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prepared = pipelines.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "def model_performace(model,X,y):\n",
    "    model.fit(X,y)\n",
    "    pred = model.predict(X)\n",
    "    scores = np.sqrt(mean_squared_error(pred,y))\n",
    "    \n",
    "    print(\"scores:\",scores)\n",
    "    print(\"Mean:\",scores.mean())\n",
    "    print(\"Standard Deviation:\",scores.std())\n",
    "    return model,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_true(ypred,ytrue):\n",
    "    df = pd.DataFrame([ypred,ytrue]).T\n",
    "    df.columns=['pred','true']\n",
    "    plt.scatter(df['pred'],df['true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(x_train, y_train)\n",
    "dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(x_train, y_train)\n",
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------------------------------------------------------------------------\n",
    "#### * It can be seen from the figure that the effect of random forest model is the best, far better than the other two models. The effect of using decision tree is the worst.\n",
    "#### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr_model,ypred = model_performace(lr,x_train_prepared, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(x_train, y_train)\n",
    "dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor() \n",
    "tree_model,ypred= model_performace(tree,x_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor()\n",
    "forest_model,y_pred = model_performace(forest,x_train_prepared,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1 = housing.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.plot.scatter(x = \"housing_median_age\", y = \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1 = housing_df1.loc[housing_df[\"population\"]<20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.plot.scatter(x = \"housing_median_age\", y = \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1[\"house_pop\"] = housing_df1[\"households\"]/housing_df1[\"population\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1 = pd.get_dummies(housing_df1, columns=[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1.columns\n",
    "housing_df1_norm = housing_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "column_names_to_normalize = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "       'median_house_value', 'house_pop']\n",
    "x = housing_df1_norm[column_names_to_normalize].values\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = housing_df1_norm.index)\n",
    "housing_df1_norm[column_names_to_normalize] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df1_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_data = housing_df1.copy()\n",
    "m1Y = m1_data['median_house_value']\n",
    "m2_data = housing_df1.copy()\n",
    "m2_data.drop(columns=['total_rooms',\n",
    "       'total_bedrooms','population','households'],inplace=True)\n",
    "m1_data.drop(columns=['median_house_value','total_rooms',\n",
    "       'total_bedrooms','population','households'],inplace=True)\n",
    "m1_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1X_train, m1X_test, m1y_train, m1y_test = train_test_split(m1_data, m1Y, test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = linear_model.LinearRegression()\n",
    "model1.fit(m1X_train, m1y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1y_pred = model1.predict(m1X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_squared_error(m1y_test, m1y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(m1y_test, m1y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(m1_data.columns,model1.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(m1y_test,m1y_pred),columns=['Actual','Predicted'])"
   ]
  },
  {
   "source": [
    "* As you can see, the sample predictions are not very good: such as the first one: it is off by (378000-326749)/378000~=13%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.score(m1X_train, m1y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_data.rename(columns={'ocean_proximity_<1H OCEAN': 'ocean_proximity1', 'ocean_proximity_NEAR BAY': 'ocean_proximity2', 'ocean_proximity_NEAR OCEAN': 'ocean_proximity3'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = smf.ols(formula='median_house_value ~ longitude + latitude + housing_median_age  + median_income + house_pop + ocean_proximity1 + ocean_proximity_INLAND + ocean_proximity2 + ocean_proximity3', data=m2_data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.summary()"
   ]
  },
  {
   "source": [
    "## Training models:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = make_pipeline(preprocessing.StandardScaler(),DecisionTreeRegressor(random_state=0))\n",
    "scores = cross_val_score(model_dt, m1X_train, m1y_train, cv=10)\n",
    "\n",
    "model_dt.fit(m1X_train, m1y_train)\n",
    "pred = model_dt.predict(m1X_test)\n",
    "\n",
    "MSE = metrics.mean_squared_error(m1y_test, pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(m1y_test, pred))\n",
    "\n",
    "dic={}\n",
    "\n",
    "dic['DecisionTree'] = (RMSE,scores.mean() *100)\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(\"---------------------\")\n",
    "print(\"MSE:\", MSE, \"RMSE:\", RMSE)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean() *100, scores.std() * 2))"
   ]
  },
  {
   "source": [
    "#### Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = make_pipeline(preprocessing.StandardScaler(),linear_model.LinearRegression())\n",
    "scores = cross_val_score(model_lr, m1X_train, m1y_train, cv=10)\n",
    "\n",
    "model_lr.fit(m1X_train, m1y_train)\n",
    "pred = model_lr.predict(m1X_test)\n",
    "\n",
    "MSE = metrics.mean_squared_error(m1y_test, pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(m1y_test, pred))\n",
    "\n",
    "dic['LinearRegression'] = (RMSE,scores.mean() *100)\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "print(\"---------------------\")\n",
    "print(\"MSE:\", MSE, \"RMSE:\", RMSE)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean() *100, scores.std() * 2))"
   ]
  },
  {
   "source": [
    "#### Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = make_pipeline(preprocessing.StandardScaler(),RandomForestRegressor(max_depth=50, random_state=0))\n",
    "scores = cross_val_score(model_rf, m1X_train, m1y_train, cv=10)\n",
    "\n",
    "model_rf.fit(m1X_train, m1y_train)\n",
    "pred = model_rf.predict(m1X_test)\n",
    "\n",
    "MSE = metrics.mean_squared_error(m1y_test, pred)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(m1y_test, pred))\n",
    "\n",
    "dic['Random Forest'] = (RMSE,scores.mean() *100)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print(\"---------------------\")\n",
    "print(\"Depth:50 Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean() *100, scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model_rf.fit(m1X_train, m1y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test.predict(m1X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(m1y_test, pred)))"
   ]
  },
  {
   "source": [
    "## Output all results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_model = pd.DataFrame(dic,index=['RMSE','Training_Accuracy'])\n",
    "accuracy_model = accuracy_model.T\n",
    "accuracy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_model.sort_values(by=['Training_Accuracy'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(8,5))\n",
    "sns.barplot(x=accuracy_model.index,y='Training_Accuracy',data=accuracy_model,ax=ax1,palette='Blues')\n",
    "plt.title(\"Models_comparision\")\n",
    "plt.savefig(\"static/images/model_comparision.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}